<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<title>Page 9</title>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<style type="text/css">
<!--
	p {margin: 0; padding: 0;}	.ft00{font-size:12px;font-family:ZENJOE+GoogleSansText-Medium;color:#80868b;}
	.ft01{font-size:17px;font-family:WGJRYK+GoogleSansText;color:#202124;}
	.ft02{font-size:24px;font-family:WGJRYK+GoogleSans;color:#202124;}
	.ft03{font-size:21px;font-family:WGJRYK+GoogleSans;color:#202124;}
	.ft04{font-size:17px;line-height:26px;font-family:WGJRYK+GoogleSansText;color:#202124;}
-->
</style>
</head>
<body bgcolor="#A0A0A0" vlink="blue" link="blue">
<div id="page9-div" style="position:relative;width:918px;height:1188px;">
<img width="918" height="1188" src="22365_3_Prompt Engineering_v7 (1)009.png" alt="background image"/>
<p style="position:absolute;top:86px;left:108px;white-space:nowrap" class="ft00">Prompt&#160;Engineering</p>
<p style="position:absolute;top:1076px;left:108px;white-space:nowrap" class="ft00">February&#160;2025</p>
<p style="position:absolute;top:1076px;left:803px;white-space:nowrap" class="ft00">9</p>
<p style="position:absolute;top:197px;left:108px;white-space:nowrap" class="ft04">Reducing the output length of the LLM doesn’t cause the LLM to become more stylistically&#160;<br/>or textually succinct in the output it creates, it just causes the LLM to stop predicting more&#160;<br/>tokens once the limit is reached. If your needs require a short output length, you’ll also&#160;<br/>possibly need to engineer your prompt to accommodate.</p>
<p style="position:absolute;top:332px;left:108px;white-space:nowrap" class="ft04">Output length restriction is especially important for some LLM prompting techniques, like&#160;<br/>ReAct, where the LLM will keep emitting useless tokens after the response you want.</p>
<p style="position:absolute;top:413px;left:108px;white-space:nowrap" class="ft04">Be aware, generating more tokens requires more computation from the LLM, leading&#160;<br/>to higher energy consumption and potentially slower response times, which leads to&#160;<br/>higher&#160;costs.</p>
<p style="position:absolute;top:542px;left:108px;white-space:nowrap" class="ft02"><b>Sampling&#160;controls</b></p>
<p style="position:absolute;top:602px;left:108px;white-space:nowrap" class="ft04">LLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the&#160;<br/>next token could be, with each token in the LLM’s vocabulary getting a probability. Those&#160;<br/>token probabilities are then sampled to determine what the next produced token will be.&#160;<br/>Temperature, top-K, and top-P are the most common configuration settings that determine&#160;<br/>how predicted token probabilities are processed to choose a single output token.</p>
<p style="position:absolute;top:788px;left:108px;white-space:nowrap" class="ft03"><b>Temperature</b></p>
<p style="position:absolute;top:845px;left:108px;white-space:nowrap" class="ft04">Temperature controls the degree of randomness in token selection. Lower temperatures&#160;<br/>are good for prompts that expect a more deterministic response, while higher temperatures&#160;<br/>can lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is&#160;</p>
</div>
</body>
</html>
